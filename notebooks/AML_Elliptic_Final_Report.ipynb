{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e187c436",
   "metadata": {},
   "source": [
    "# Anti-Money Laundering on the Elliptic Bitcoin Dataset\n",
    "## End-to-end pipeline: preprocessing → EDA → baselines → GNNs → tuning → evaluation\n",
    "\n",
    "**Author:** AmirAli Hosseini Abrishami  \n",
    "**Repository:** `aml-elliptic` (see `results/logs/*` for run manifests + commit hashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb076f",
   "metadata": {},
   "source": [
    "## 0) Reproducibility and execution controls\n",
    "\n",
    "\n",
    "- **Artifact-only:** load and display precomputed outputs under `results/`.\n",
    "\n",
    "All phases write:\n",
    "- a **run manifest** (`results/logs/phaseXX_*.json`) with timestamps, commit hash, command, environment, and input file metadata.\n",
    "- **metrics JSON** under `results/metrics/`\n",
    "- **plots** under `results/visualizations/`\n",
    "\n",
    "Use `RUN_HEAVY=False` unless the grader explicitly asks to reproduce everything live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Controls ===\n",
    "RUN_HEAVY = False   # True = re-run phases (may take time / require GPU). False = load existing artifacts.\n",
    "DEVICE = \"cuda\"     # \"cuda\" or \"cpu\" (only relevant if RUN_HEAVY=True).\n",
    "RANDOM_SEED = 42    # must match your pipeline's seed if you fixed one\n",
    "\n",
    "# === Paths (assumes you run this notebook from repo root) ===\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA = ROOT / \"data\"\n",
    "RESULTS = ROOT / \"results\"\n",
    "REPORTS = ROOT / \"reports\"\n",
    "SRC = ROOT / \"src\"\n",
    "\n",
    "METRICS = RESULTS / \"metrics\"\n",
    "VIZ = RESULTS / \"visualizations\"\n",
    "LOGS = RESULTS / \"logs\"\n",
    "ARTIFACTS = RESULTS / \"model_artifacts\"\n",
    "PRED = RESULTS / \"predictions\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "for p in [DATA, RESULTS, METRICS, VIZ, LOGS, ARTIFACTS, PRED]:\n",
    "    print(f\"{p}: {'OK' if p.exists() else 'MISSING'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e59976",
   "metadata": {},
   "source": [
    "## Helper utilities (safe for missing files)\n",
    "These helpers keep the notebook robust during presentation (e.g., if some optional artifacts are missing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79729536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_json(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {path}\")\n",
    "        return None\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def show_image(path: Path, title: str | None = None):\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {path}\")\n",
    "        return\n",
    "    import matplotlib.image as mpimg\n",
    "    img = mpimg.imread(path)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def list_dir(path: Path, glob=\"*\"):\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {path}\")\n",
    "        return []\n",
    "    out = sorted(path.glob(glob))\n",
    "    for p in out:\n",
    "        print(p.relative_to(ROOT))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33137",
   "metadata": {},
   "source": [
    "## 1) Project structure\n",
    "\n",
    "The repository is organized into a **phase-based, reproducible pipeline**:\n",
    "\n",
    "- `src/` : implementation (phase scripts + shared utilities)\n",
    "- `data/raw/` : original Elliptic CSV files\n",
    "- `data/processed/` : deterministic processed artifacts from Phase 01\n",
    "- `results/metrics/` : machine-readable JSON summaries (per phase)\n",
    "- `results/visualizations/` : exported plots used in this report\n",
    "- `results/model_artifacts/` : serialized trained models and thresholds\n",
    "- `results/logs/` : run manifests + artifact inventories (provenance)\n",
    "\n",
    "The full tree below is the submission snapshot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2cff9",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "├── data\n",
    "│   ├── processed\n",
    "│   │   ├── elliptic_edges.csv\n",
    "│   │   ├── elliptic_edges_kept.csv\n",
    "│   │   ├── elliptic_edges_kept.parquet\n",
    "│   │   ├── elliptic_edges.parquet\n",
    "│   │   ├── elliptic_full.csv\n",
    "│   │   ├── elliptic_full.parquet\n",
    "│   │   ├── elliptic_labeled.csv\n",
    "│   │   ├── elliptic_labeled.parquet\n",
    "│   │   └── node_index.parquet\n",
    "│   └── raw\n",
    "│       ├── elliptic_txs_classes.csv\n",
    "│       ├── elliptic_txs_edgelist.csv\n",
    "│       └── elliptic_txs_features.csv\n",
    "├── notebooks\n",
    "│   └──AML_Elliptic_Final_Report.ipynb\n",
    "├── reports\n",
    "│   ├── final_report.md\n",
    "│   ├── phase0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c92269",
   "metadata": {},
   "source": [
    "### 1.1 Implementation entry points\n",
    "\n",
    "| Component | File | Purpose |\n",
    "|---|---|---|\n",
    "| Phase runner | `src/main.py` | CLI entry (`python -m src.main --phase N`), dispatches phases and passes config/device. |\n",
    "| Config | `src/config.py` | Central experiment configuration, paths, default parameters, seed. |\n",
    "| Data utilities | `src/data.py` | Load Elliptic raw/processed data, label mapping, feature-column selection (AF/LF). |\n",
    "| Graph builder | `src/graph_data.py` | Build PyG graph object, masks, and normalization using training-labeled stats only. |\n",
    "| Metrics | `src/eval.py` | Compute illicit precision/recall/F1, micro-F1, confusion matrices. |\n",
    "| Plotting | `src/viz.py` | Export plots used throughout (`results/visualizations/*`). |\n",
    "| Logging | `src/runlog.py` | Write run manifests and (optionally) artifact inventories. |\n",
    "| Phase 01 | `src/phase01_preprocessing.py` | Raw → processed tables + integrity checks + summaries. |\n",
    "| Phase 02 | `src/phase02_eda.py` | EDA metrics + plot exports (labels, time, degree, feature proxies). |\n",
    "| Phase 02b | `src/phase02b_normalization_viz.py` | Normalization diagnostic plots (hist/PCA). |\n",
    "| Phase 03 | `src/phase03_models.py` | Tabular baselines + GNN baselines + embedding augmentation. |\n",
    "| Phase 04 | `src/phase04_tuning.py` | Temporal CV HPO + threshold tuning + calibration/PR/confusion plots. |\n",
    "| Phase 05 | `src/phase05_eval_infer.py` | Final evaluation + prediction exports + final summary plot. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0a822",
   "metadata": {},
   "source": [
    "## 2) Dataset and task\n",
    "\n",
    "**Task:** binary node classification (illicit vs licit) on the **Elliptic** Bitcoin transaction graph with **temporal generalization**.\n",
    "\n",
    "Each node represents a transaction `txId` with a time step `time_step ∈ {1..49}` and a feature vector (`feat_*`).  \n",
    "Edges represent flows between transactions (`txId1 → txId2`). A large fraction of nodes are **unlabeled** (`class_raw = unknown`), but are still used as **structural context** for graph learning.\n",
    "\n",
    "**Label encoding (used across all phases):**\n",
    "- `1` = illicit (positive class)\n",
    "- `0` = licit (negative class)\n",
    "- `-1` = unknown/unlabeled (excluded from supervised loss/metrics)\n",
    "\n",
    "**Primary evaluation protocol:** chronological split by `time_step` (train on earlier steps, test on later steps) to prevent temporal leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7867c5b",
   "metadata": {},
   "source": [
    "### 2.1 Dataset summary (validated in Phase 01)\n",
    "\n",
    "Canonical properties (as validated by `phase01_data_summary.json` and contract checks):\n",
    "- Nodes: **203,769**\n",
    "- Directed edges: **234,355** (100% node coverage)\n",
    "- Time steps: **49** (min=1, max=49)\n",
    "- Features: **165 numeric features** (`feat_0 … feat_164`) + identifiers (`txId`, `time_step`)\n",
    "- Labels:\n",
    "  - Labeled nodes: **46,564** (= 42,019 licit + 4,545 illicit)\n",
    "  - Unlabeled nodes: **157,205**\n",
    "\n",
    "Key implications:\n",
    "- Strong class imbalance (illicit ≪ licit) → use class weights + illicit-focused metrics.\n",
    "- Temporal split is mandatory for credible performance estimates.\n",
    "- Graph is sparse and heavy-tailed in degree → shallow, regularized GNNs are appropriate baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d631aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, JSON\n",
    "\n",
    "def read_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def show_json(obj: dict, *, expanded: bool = False):\n",
    "    # Interactive collapsible view (best for nested dicts)\n",
    "    display(JSON(obj, expanded=expanded))\n",
    "\n",
    "def flatten(d: dict, parent_key: str = \"\", sep: str = \".\") -> dict:\n",
    "    # Flatten nested dicts for table display\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else str(k)\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def df_kv(obj: dict, title: str | None = None, max_rows: int = 200):\n",
    "    # Key/value table from a dict (flat)\n",
    "    flat = flatten(obj)\n",
    "    df = (\n",
    "        pd.DataFrame({\"key\": list(flat.keys()), \"value\": list(flat.values())})\n",
    "        .sort_values(\"key\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    if title:\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "    display(df.head(max_rows))\n",
    "\n",
    "def section(title: str):\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "def subsection(title: str):\n",
    "    display(Markdown(f\"### {title}\"))\n",
    "\n",
    "def show_phase01_pretty(phase01: dict):\n",
    "    # 1) interactive full object (collapsed by default)\n",
    "    section(\"Phase 01 summary (interactive)\")\n",
    "    show_json(phase01, expanded=False)\n",
    "\n",
    "    # 2) high-signal summaries as clean tables\n",
    "    section(\"Phase 01 key metrics\")\n",
    "\n",
    "    # a) saved artifacts\n",
    "    saved_keys = [k for k in phase01.keys() if k.startswith(\"saved_\")]\n",
    "    saved = []\n",
    "    for k in saved_keys:\n",
    "        v = phase01[k]\n",
    "        if isinstance(v, list) and len(v) >= 1:\n",
    "            path = v[0]\n",
    "            meta = v[1] if len(v) > 1 and isinstance(v[1], dict) else {}\n",
    "        else:\n",
    "            path, meta = v, {}\n",
    "        saved.append({\"artifact\": k, \"path\": path, **meta})\n",
    "    if saved:\n",
    "        subsection(\"Saved artifacts\")\n",
    "        display(pd.DataFrame(saved).sort_values(\"artifact\").reset_index(drop=True))\n",
    "\n",
    "    # b) raw input integrity\n",
    "    raw = phase01.get(\"raw_inputs\", {})\n",
    "    if raw:\n",
    "        subsection(\"Raw inputs (integrity)\")\n",
    "        rows = []\n",
    "        for name, info in raw.items():\n",
    "            rows.append({\n",
    "                \"input\": name,\n",
    "                \"path\": info.get(\"path\"),\n",
    "                \"exists\": info.get(\"exists\"),\n",
    "                \"bytes\": info.get(\"bytes\"),\n",
    "                \"mtime_utc\": info.get(\"mtime_utc\"),\n",
    "                \"sha256\": info.get(\"sha256\"),\n",
    "            })\n",
    "        df = pd.DataFrame(rows).sort_values(\"input\").reset_index(drop=True)\n",
    "        display(df)\n",
    "\n",
    "    # c) counts + label distribution\n",
    "    counts = {\n",
    "        \"nodes_total\": phase01.get(\"full_rows\"),\n",
    "        \"nodes_labeled\": phase01.get(\"labeled_rows\"),\n",
    "        \"nodes_unlabeled\": phase01.get(\"unknown_rows\"),\n",
    "        \"time_steps_unique\": phase01.get(\"time_steps_unique\"),\n",
    "        \"feature_cols\": phase01.get(\"num_feature_cols\"),\n",
    "        \"edges_kept\": phase01.get(\"edges\", {}).get(\"kept_rows\"),\n",
    "    }\n",
    "    label_labeled = phase01.get(\"label_counts_labeled_mapped\", {})\n",
    "    illicit = label_labeled.get(\"1\", None)\n",
    "    labeled = phase01.get(\"labeled_rows\", None)\n",
    "    if illicit is not None and labeled:\n",
    "        counts[\"illicit_fraction_labeled\"] = illicit / labeled\n",
    "\n",
    "    subsection(\"Core dataset counts\")\n",
    "    display(pd.DataFrame([counts]))\n",
    "\n",
    "    if label_labeled:\n",
    "        subsection(\"Label counts (labeled subset)\")\n",
    "        display(\n",
    "            pd.DataFrame([label_labeled])\n",
    "            .rename(columns={\"0\": \"licit(0)\", \"1\": \"illicit(1)\"})\n",
    "        )\n",
    "\n",
    "    # d) edge stats (compact)\n",
    "    edge_stats = phase01.get(\"edge_stats_kept_induced\", {})\n",
    "    if edge_stats:\n",
    "        subsection(\"Edge stats (kept, induced)\")\n",
    "        keep = [\n",
    "            \"edges_kept\", \"self_loops\", \"duplicate_edges_directed\",\n",
    "            \"deg_min\", \"deg_median\", \"deg_mean\", \"deg_max\",\n",
    "            \"isolated_nodes\"\n",
    "        ]\n",
    "        compact = {k: edge_stats.get(k) for k in keep if k in edge_stats}\n",
    "        display(pd.DataFrame([compact]))\n",
    "\n",
    "    # e) coverage\n",
    "    cov = phase01.get(\"edge_coverage\", {})\n",
    "    if cov:\n",
    "        subsection(\"Edge coverage\")\n",
    "        display(pd.DataFrame([cov]))\n",
    "\n",
    "# --- Usage ---\n",
    "phase01 = read_json(METRICS / \"phase01_data_summary.json\")\n",
    "show_phase01_pretty(phase01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f393b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight sanity-check (optional): load processed tables if present\n",
    "import pandas as pd\n",
    "\n",
    "full_pq = DATA / \"processed\" / \"elliptic_full.parquet\"\n",
    "lab_pq  = DATA / \"processed\" / \"elliptic_labeled.parquet\"\n",
    "\n",
    "df_full = pd.read_parquet(full_pq) if full_pq.exists() else None\n",
    "df_lab  = pd.read_parquet(lab_pq)  if lab_pq.exists() else None\n",
    "\n",
    "if df_full is not None:\n",
    "    display(df_full.head(3))\n",
    "    print(\"df_full shape:\", df_full.shape)\n",
    "    print(\"time_step:\", int(df_full.time_step.min()), \"→\", int(df_full.time_step.max()), \"unique:\", df_full.time_step.nunique())\n",
    "    print(\"class value counts:\", df_full[\"class\"].value_counts().to_dict())\n",
    "\n",
    "if df_lab is not None:\n",
    "    display(df_lab.head(3))\n",
    "    print(\"df_labeled shape:\", df_lab.shape)\n",
    "    print(\"class value counts:\", df_lab[\"class\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682654dc",
   "metadata": {},
   "source": [
    "### 2.2 Executive summary (headline performance)\n",
    "\n",
    "This table is what the grader typically cares about first: **test-set illicit precision/recall/F1** under the **temporal split** (future time steps held out).\n",
    "\n",
    "The notebook will load `results/metrics/phase05_eval_report.json` if present; otherwise it falls back to the values documented in `reports/phase05_evaluation.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headline metrics (Phase 05 if available; fallback otherwise)\n",
    "from math import isnan\n",
    "\n",
    "\n",
    "\n",
    "phase05 = read_json(METRICS / \"phase05_eval_report.json\")\n",
    "rows = []\n",
    "\n",
    "def add_row(name, d):\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"precision_illicit\": d.get(\"precision_illicit\"),\n",
    "        \"recall_illicit\": d.get(\"recall_illicit\"),\n",
    "        \"f1_illicit\": d.get(\"f1_illicit\"),\n",
    "        \"f1_micro\": d.get(\"f1_micro\"),\n",
    "    })\n",
    "\n",
    "if phase05 and \"phase04_tabular_eval\" in phase05:\n",
    "    # Tabular\n",
    "    te = phase05[\"phase04_tabular_eval\"]\n",
    "    for mode in (\"AF\", \"LF\"):\n",
    "        if mode in te:\n",
    "            if \"lr\" in te[mode] and \"metrics\" in te[mode][\"lr\"]:\n",
    "                add_row(f\"LR_{mode}\", te[mode][\"lr\"][\"metrics\"])\n",
    "            if \"rf\" in te[mode] and \"metrics\" in te[mode][\"rf\"]:\n",
    "                add_row(f\"RF_{mode}\", te[mode][\"rf\"][\"metrics\"])\n",
    "    # GNN (AF)\n",
    "    if \"gcn_AF\" in phase05 and \"metrics\" in phase05[\"gcn_AF\"]:\n",
    "        add_row(\"GCN_AF\", phase05[\"gcn_AF\"][\"metrics\"])\n",
    "    if \"skip_gcn_AF\" in phase05 and \"metrics\" in phase05[\"skip_gcn_AF\"]:\n",
    "        add_row(\"SkipGCN_AF\", phase05[\"skip_gcn_AF\"][\"metrics\"])\n",
    "else:\n",
    "    print(\"not found\")\n",
    "\n",
    "df_headline = pd.DataFrame(rows).sort_values(\"f1_illicit\", ascending=False)\n",
    "df_headline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c4bf8",
   "metadata": {},
   "source": [
    "## 3) Phase 01 — Preprocessing (raw → processed tables)\n",
    "\n",
    "**Goal:** build a deterministic, validated, and reproducible dataset foundation for all later phases.\n",
    "\n",
    "### What Phase 01 does\n",
    "1. Load the three raw CSV files:\n",
    "   - features (`elliptic_txs_features.csv`)\n",
    "   - classes (`elliptic_txs_classes.csv`)\n",
    "   - edge list (`elliptic_txs_edgelist.csv`)\n",
    "2. Construct a canonical **full node table** (`elliptic_full`) aligned by `txId`.\n",
    "3. Map raw labels (`{1,2,unknown}`) → internal labels (`{1,0,-1}`).\n",
    "4. Enforce a strict **data contract**:\n",
    "   - one row per `txId`\n",
    "   - `time_step` spans 1..49\n",
    "   - feature columns are finite (no NaN/inf)\n",
    "   - node ordering is deterministic (sorted `txId`)\n",
    "5. Export stable artifacts (Parquet-first) used by all downstream phases:\n",
    "   - `data/processed/elliptic_full.(parquet|csv)`\n",
    "   - `data/processed/elliptic_labeled.(parquet|csv)`\n",
    "   - `data/processed/node_index.(parquet|csv)` (canonical `txId → node_idx`)\n",
    "   - `data/processed/elliptic_edges_kept.(parquet|csv)` (filtered + deduplicated + sorted)\n",
    "6. Write integrity metadata (SHA256, bytes, mtimes) into `results/metrics/phase01_data_summary.json`.\n",
    "\n",
    "### Why it matters\n",
    "- Prevents silent misalignment between node features, labels, and edges.\n",
    "- Makes later results reproducible and auditable (hashes + manifests).\n",
    "- Catches data corruption early, before expensive model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67379c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show Phase 01 run manifest (adjust filename if needed)\n",
    "list_dir(LOGS, \"phase01_*.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb8ab4",
   "metadata": {},
   "source": [
    "## 4) Phase 02 — EDA (graph + labels + temporal behavior)\n",
    "\n",
    "**Goal:** characterize the dataset and produce reusable plots that justify modeling decisions.\n",
    "\n",
    "### What is measured\n",
    "- **Label imbalance** and its temporal behavior (counts + illicit ratio over `time_step`)\n",
    "- **Graph structure** (degree distribution; heavy-tail behavior; subgroup comparisons)\n",
    "- **Selection effects** (differences between labeled vs unlabeled nodes)\n",
    "- **Feature signal proxies** (rank features by class-conditional mean differences)\n",
    "\n",
    "### Key takeaways (from exported plots/metrics)\n",
    "- Illicit is a small minority class → optimize/report **illicit precision/recall/F1**.\n",
    "- Degree distribution is sparse and heavy-tailed → simple GNN baselines are justified, but careful regularization is required.\n",
    "- Labeled vs unlabeled structural differences suggest potential selection bias.\n",
    "- Temporal behavior motivates strict chronological evaluation (avoid leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display key Phase 02 plots\n",
    "eda_dir = VIZ / \"phase02_eda\"\n",
    "list_dir(eda_dir, \"*.png\")\n",
    "\n",
    "# Recommended sequence to show during presentation\n",
    "for name in [\n",
    "    \"class_distribution.png\",\n",
    "    \"nodes_per_timestep_full.png\",\n",
    "    \"nodes_per_timestep_labeled.png\",\n",
    "    \"illicit_ratio_over_time.png\",\n",
    "    \"degree_distribution_log.png\",\n",
    "    \"top_feature_mean_diff.png\",\n",
    "]:\n",
    "    show_image(eda_dir / name, title=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459f381",
   "metadata": {},
   "source": [
    "## 4b) Phase 02b — Normalization visualization (if included)\n",
    "\n",
    "**Goal:** sanity-check how feature normalization changes distributions and geometry (used later in LR and GNN training).\n",
    "\n",
    "Artifacts are under `results/visualizations/normalizedData/`:\n",
    "- Feature histograms before/after normalization\n",
    "- PCA projection before/after normalization\n",
    "- Per-feature mean/std summary (`normalized_stats_AF.json`)\n",
    "\n",
    "**Important leakage rule (applied later):** normalization statistics must be computed from **training-only** data (or training-labeled nodes for GNNs), then applied to validation/test/all nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "norm_dir = VIZ / \"normalizedData\"\n",
    "list_dir(norm_dir, \"*.png\")\n",
    "\n",
    "for name in [\n",
    "    \"AF_train_feature_hist_pre.png\",\n",
    "    \"AF_train_feature_hist_post.png\",\n",
    "    \"AF_train_pca_pre_norm.png\",\n",
    "    \"AF_train_pca_post_norm.png\",\n",
    "    \"AF_means_hist.png\",\n",
    "    \"AF_stds_hist.png\",\n",
    "]:\n",
    "    show_image(norm_dir / name, title=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae809e",
   "metadata": {},
   "source": [
    "## 5) Phase 03 — Model selection (baselines vs GNN)\n",
    "\n",
    "**Goal:** establish credible baselines and compare feature sets and modeling families under a leakage-safe temporal protocol.\n",
    "\n",
    "### Experimental protocol\n",
    "- **Temporal split (primary):** train on early time steps, test on later time steps.\n",
    "  - Tabular: train steps **1..34**, test steps **35..49**.\n",
    "  - GNN: train-only **1..31**, validation **32..34**, test **35..49**.\n",
    "- **Target metric:** illicit-class precision/recall/F1 (positive class = illicit).\n",
    "- **Class imbalance:** class weights computed from training-labeled nodes.\n",
    "- **Leakage avoidance:**\n",
    "  - forward-chaining CV for tabular hyperparameter search\n",
    "  - train-only normalization for LR and GNN\n",
    "\n",
    "### Models evaluated\n",
    "- **Tabular baselines:** Logistic Regression (LR) and Random Forest (RF) on:\n",
    "  - AF = all features\n",
    "  - LF = local features\n",
    "- **Graph baselines (AF):** GCN and SkipGCN trained on the full graph (unlabeled nodes provide message-passing context; loss/metrics use labeled nodes only).\n",
    "- **Embedding augmentation:** RF trained on `features ⊕ GNN embeddings` to test whether graph representations add signal beyond engineered features.\n",
    "\n",
    "### Key Phase 03 results (model selection)\n",
    "- Best illicit-F1: **RF (AF)** ≈ **0.80** (strong precision + recall).\n",
    "- LR achieves high recall but poor precision (many false positives).\n",
    "- GNN baselines reach moderate illicit-F1 (~0.51–0.52) on the test window under this protocol.\n",
    "- Embedding augmentation yields extremely high precision but does not surpass RF(AF) in illicit-F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Results leaderboard (single table)\n",
    "# =========================\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load saved Phase 03 metrics\n",
    "baselines_AF = read_json(METRICS / \"baselines_AF.json\")\n",
    "baselines_LF = read_json(METRICS / \"baselines_LF.json\")\n",
    "gnn_AF = read_json(METRICS / \"gnn_AF.json\")\n",
    "\n",
    "def _cm_to_counts(cm):\n",
    "    # cm = [[TN, FP],[FN, TP]]\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    return {\"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp}\n",
    "\n",
    "def rows_from_baselines(b: dict, fallback_fm: str):\n",
    "    rows = []\n",
    "    fm = b.get(\"feature_mode\", fallback_fm)\n",
    "    for model_key, pretty in [(\"logreg\", \"LR\"), (\"random_forest\", \"RF\")]:\n",
    "        if model_key not in b:\n",
    "            continue\n",
    "        m = b[model_key]\n",
    "        r = {\n",
    "            \"group\": \"tabular\",\n",
    "            \"feature_mode\": fm,\n",
    "            \"model\": pretty,\n",
    "            \"precision_illicit\": m.get(\"precision_illicit\"),\n",
    "            \"recall_illicit\": m.get(\"recall_illicit\"),\n",
    "            \"f1_illicit\": m.get(\"f1_illicit\"),\n",
    "            \"f1_micro\": m.get(\"f1_micro\"),\n",
    "        }\n",
    "        if \"cm\" in m:\n",
    "            r.update(_cm_to_counts(m[\"cm\"]))\n",
    "        rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def rows_from_gnn(g: dict, fallback_fm: str):\n",
    "    rows = []\n",
    "    fm = g.get(\"feature_mode\", fallback_fm)\n",
    "    model_map = [(\"gcn\", \"GCN\"), (\"skip_gcn\", \"SkipGCN\"), (\"rf_on_gnn_embeddings\", \"RF-on-Embeddings\")]\n",
    "    for k, pretty in model_map:\n",
    "        if k not in g:\n",
    "            continue\n",
    "        block = g[k]\n",
    "        m = block.get(\"test_metrics\", block)  # sometimes metrics are stored directly\n",
    "        r = {\n",
    "            \"group\": \"gnn\",\n",
    "            \"feature_mode\": fm,\n",
    "            \"model\": pretty,\n",
    "            \"precision_illicit\": m.get(\"precision_illicit\"),\n",
    "            \"recall_illicit\": m.get(\"recall_illicit\"),\n",
    "            \"f1_illicit\": m.get(\"f1_illicit\"),\n",
    "            \"f1_micro\": m.get(\"f1_micro\"),\n",
    "        }\n",
    "        if \"cm\" in m:\n",
    "            r.update(_cm_to_counts(m[\"cm\"]))\n",
    "        rows.append(r)\n",
    "    return rows\n",
    "\n",
    "# Build leaderboard\n",
    "rows = []\n",
    "rows += rows_from_baselines(baselines_AF, \"AF\")\n",
    "rows += rows_from_baselines(baselines_LF, \"LF\")\n",
    "rows += rows_from_gnn(gnn_AF, \"AF\")\n",
    "\n",
    "df_models = pd.DataFrame(rows)\n",
    "\n",
    "# Keep only relevant columns (in order)\n",
    "cols = [\"group\",\"feature_mode\",\"model\",\"precision_illicit\",\"recall_illicit\",\"f1_illicit\",\"f1_micro\",\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "df_models = df_models[[c for c in cols if c in df_models.columns]]\n",
    "\n",
    "# Sort by key metric (desc), then by model name for stable ordering\n",
    "sort_cols = [\"f1_illicit\"] if \"f1_illicit\" in df_models.columns else [\"f1_micro\"]\n",
    "df_models = df_models.sort_values(sort_cols + [\"group\",\"feature_mode\",\"model\"], ascending=[False, True, True, True]).reset_index(drop=True)\n",
    "\n",
    "display(\n",
    "    df_models.style.format({\n",
    "        \"precision_illicit\": \"{:.3f}\",\n",
    "        \"recall_illicit\": \"{:.3f}\",\n",
    "        \"f1_illicit\": \"{:.3f}\",\n",
    "        \"f1_micro\": \"{:.3f}\",\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Experimental context (class weights + graph / split sizes)\n",
    "# =========================\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Class weights (tabular)\n",
    "# In your JSON they appear as a dict like {\"0\": w0, \"1\": w1} or {0: w0, 1: w1}\n",
    "def class_weights_table(b: dict, fallback_fm: str):\n",
    "    fm = b.get(\"feature_mode\", fallback_fm)\n",
    "    cw = b.get(\"class_weights\", {})\n",
    "    # normalize keys to strings for display\n",
    "    row = {\"feature_mode\": fm}\n",
    "    for k, v in cw.items():\n",
    "        row[str(k)] = v\n",
    "    return row\n",
    "\n",
    "cw = pd.DataFrame([\n",
    "    class_weights_table(baselines_AF, \"AF\"),\n",
    "    class_weights_table(baselines_LF, \"LF\"),\n",
    "])\n",
    "display(cw)\n",
    "\n",
    "# Graph + split summary (gnn)\n",
    "g = gnn_AF\n",
    "graph_row = (g.get(\"graph\", {}) or {}).copy()\n",
    "steps_row = {\n",
    "    \"train_steps\": f\"{min(g['train_steps'])}..{max(g['train_steps'])} (n={len(g['train_steps'])})\" if \"train_steps\" in g else None,\n",
    "    \"val_steps\":   f\"{min(g['val_steps'])}..{max(g['val_steps'])} (n={len(g['val_steps'])})\" if \"val_steps\" in g else None,\n",
    "    \"test_steps\":  f\"{min(g['test_steps'])}..{max(g['test_steps'])} (n={len(g['test_steps'])})\" if \"test_steps\" in g else None,\n",
    "    \"device\": g.get(\"device\"),\n",
    "}\n",
    "context = pd.DataFrame([{**graph_row, **steps_row}])\n",
    "display(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce33ce",
   "metadata": {},
   "source": [
    "## 6) Phase 04 — Hyperparameter tuning + thresholding + calibration\n",
    "\n",
    "**Goal:** produce final tuned **tabular** baselines under temporal CV, then choose an operational threshold without test leakage.\n",
    "\n",
    "### What Phase 04 adds on top of Phase 03\n",
    "- **Leakage-safe hyperparameter optimization:** forward-chaining CV within the training time window.\n",
    "- **Threshold tuning:** choose a decision threshold `t` for `p(illicit)` using a **tail slice inside training** (not the test set), optimizing illicit-F1.\n",
    "- **Diagnostics:** PR curves, threshold sweeps, calibration curves, confusion matrices.\n",
    "\n",
    "### Why threshold tuning is necessary\n",
    "Under severe class imbalance, a default `0.5` threshold is rarely optimal.  \n",
    "Phase 04 explicitly selects thresholds aligned with the project’s target metric (illicit-F1) using only past data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phase04_dir = VIZ / \"phase04\"\n",
    "list_dir(phase04_dir, \"*.png\")\n",
    "\n",
    "# Recommended minimal set\n",
    "for name in [\n",
    "    \"pr_rf_AF_cuml.png\",\n",
    "    \"pr_lr_AF_cuml.png\",\n",
    "    \"thr_sweep_rf_AF_cuml.png\",\n",
    "    \"cm_rf_AF_cuml.png\",\n",
    "    \"cm_lr_AF_cuml.png\",\n",
    "    \"calib_rf_AF_cuml.png\",\n",
    "]:\n",
    "    show_image(phase04_dir / name, title=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20074173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 04 tuning summary table (reads threshold float correctly)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "def fmt3(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return \"\"\n",
    "    try:\n",
    "        return f\"{float(x):.3f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def read_json(path: Path):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def summarize_phase04_mode(tuning: dict, mode: str):\n",
    "    \"\"\"\n",
    "    Schema:\n",
    "      tuning keys: feature_mode, tabular_backend, split, threshold_tune_tail_steps, class_weights_train, cv, logreg, random_forest\n",
    "      tuning[model] keys: best_hparams, best_cv_f1_illicit, threshold (float), tail_f1_illicit, test, trials_top10\n",
    "    \"\"\"\n",
    "    if not isinstance(tuning, dict):\n",
    "        return []\n",
    "\n",
    "    out = []\n",
    "    for key, label in [(\"logreg\", \"LR\"), (\"random_forest\", \"RF\")]:\n",
    "        block = tuning.get(key)\n",
    "        if not isinstance(block, dict):\n",
    "            continue\n",
    "\n",
    "        test = block.get(\"test\") if isinstance(block.get(\"test\"), dict) else {}\n",
    "        thr_val = block.get(\"threshold\")\n",
    "\n",
    "        out.append({\n",
    "            \"mode\": mode,\n",
    "            \"model\": label,\n",
    "            \"backend\": tuning.get(\"tabular_backend\"),\n",
    "            \"best_cv_f1_illicit\": block.get(\"best_cv_f1_illicit\"),\n",
    "            \"tuned_threshold\": thr_val,\n",
    "            \"tail_f1_illicit\": block.get(\"tail_f1_illicit\"),\n",
    "            \"test_precision_illicit\": (test or {}).get(\"precision_illicit\"),\n",
    "            \"test_recall_illicit\": (test or {}).get(\"recall_illicit\"),\n",
    "            \"test_f1_illicit\": (test or {}).get(\"f1_illicit\"),\n",
    "            \"test_f1_micro\": (test or {}).get(\"f1_micro\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# --- load files ---\n",
    "af_path  = Path(METRICS) / \"phase04_tuning_AF_cuml.json\"\n",
    "lf_path  = Path(METRICS) / \"phase04_tuning_LF_cuml.json\"\n",
    "idx_path = Path(METRICS) / \"phase04_tuning_index_cuml.json\"\n",
    "\n",
    "tuning_AF  = read_json(af_path)\n",
    "tuning_LF  = read_json(lf_path)\n",
    "tuning_idx = read_json(idx_path)\n",
    "\n",
    "# --- prefer index if present; your index uses \"modes\" ---\n",
    "rows = []\n",
    "if isinstance(tuning_idx, dict) and isinstance(tuning_idx.get(\"modes\"), dict):\n",
    "    modes = tuning_idx[\"modes\"]\n",
    "    if isinstance(modes.get(\"AF\"), dict):\n",
    "        rows += summarize_phase04_mode(modes[\"AF\"], \"AF\")\n",
    "    if isinstance(modes.get(\"LF\"), dict):\n",
    "        rows += summarize_phase04_mode(modes[\"LF\"], \"LF\")\n",
    "\n",
    "# --- fallback to standalone files ---\n",
    "if not rows:\n",
    "    if isinstance(tuning_AF, dict):\n",
    "        rows += summarize_phase04_mode(tuning_AF, \"AF\")\n",
    "    if isinstance(tuning_LF, dict):\n",
    "        rows += summarize_phase04_mode(tuning_LF, \"LF\")\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"Could not parse Phase 04 tuning JSONs.\")\n",
    "else:\n",
    "    df_tuning = pd.DataFrame(rows).sort_values([\"mode\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "display(\n",
    "    df_tuning.style.format({\n",
    "        \"best_cv_f1_illicit\": fmt3,\n",
    "        \"tuned_threshold\": fmt3,\n",
    "        \"tail_f1_illicit\": fmt3,\n",
    "        \"test_precision_illicit\": fmt3,\n",
    "        \"test_recall_illicit\": fmt3,\n",
    "        \"test_f1_illicit\": fmt3,\n",
    "        \"test_f1_micro\": fmt3,\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1339c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact Phase 04 split / class-weights / CV summaries (handles int val_steps & int folds)\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Any, Dict, Mapping, Optional, List\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def _as_list(x: Any) -> List[Any]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    return [x]  # scalar -> singleton list\n",
    "\n",
    "def _range_str(x: Any) -> Optional[str]:\n",
    "    xs = _as_list(x)\n",
    "    if not xs:\n",
    "        return None\n",
    "    try:\n",
    "        return f\"{min(xs)}..{max(xs)} (n={len(xs)})\"\n",
    "    except Exception:\n",
    "        return f\"n={len(xs)}\"\n",
    "\n",
    "def compact_split(tuning: Optional[Mapping[str, Any]], mode: str) -> Optional[Dict[str, Any]]:\n",
    "    if not isinstance(tuning, Mapping):\n",
    "        return None\n",
    "    split = tuning.get(\"split\")\n",
    "    split = split if isinstance(split, Mapping) else {}\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"backend\": tuning.get(\"tabular_backend\"),\n",
    "        \"threshold_tune_tail_steps\": _range_str(tuning.get(\"threshold_tune_tail_steps\")),\n",
    "        \"train_steps\": _range_str(split.get(\"train_steps\")),\n",
    "        \"test_steps\": _range_str(split.get(\"test_steps\")),\n",
    "    }\n",
    "\n",
    "def compact_class_weights(tuning: Optional[Mapping[str, Any]], mode: str) -> Optional[Dict[str, Any]]:\n",
    "    if not isinstance(tuning, Mapping):\n",
    "        return None\n",
    "    cw = tuning.get(\"class_weights_train\")\n",
    "    cw = cw if isinstance(cw, Mapping) else {}\n",
    "    return {\"mode\": mode, \"w0\": cw.get(\"0\"), \"w1\": cw.get(\"1\")}\n",
    "\n",
    "def compact_cv(tuning: Optional[Mapping[str, Any]], mode: str) -> Optional[Dict[str, Any]]:\n",
    "    if not isinstance(tuning, Mapping):\n",
    "        return None\n",
    "    cv = tuning.get(\"cv\")\n",
    "    cv = cv if isinstance(cv, Mapping) else {}\n",
    "\n",
    "    # your JSON: val_steps is int, folds is int (count)\n",
    "    val_steps = cv.get(\"val_steps\")\n",
    "    folds = cv.get(\"folds\")\n",
    "\n",
    "    folds_count = folds if isinstance(folds, int) else (len(folds) if isinstance(folds, (list, tuple, dict)) else None)\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"n_splits\": cv.get(\"n_splits\"),\n",
    "        \"val_steps\": _range_str(val_steps),\n",
    "        \"folds\": folds_count,\n",
    "    }\n",
    "\n",
    "rows_split = [r for r in (compact_split(tuning_AF, \"AF\"), compact_split(tuning_LF, \"LF\")) if r]\n",
    "rows_cw    = [r for r in (compact_class_weights(tuning_AF, \"AF\"), compact_class_weights(tuning_LF, \"LF\")) if r]\n",
    "rows_cv    = [r for r in (compact_cv(tuning_AF, \"AF\"), compact_cv(tuning_LF, \"LF\")) if r]\n",
    "\n",
    "display(pd.DataFrame(rows_split))\n",
    "display(pd.DataFrame(rows_cw))\n",
    "display(pd.DataFrame(rows_cv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6e823",
   "metadata": {},
   "source": [
    "## 7) Phase 05 — Final evaluation + inference outputs\n",
    "\n",
    "**Goal:** reproduce final test-set metrics for:\n",
    "- tuned tabular models from Phase 04 (AF + LF), using saved scalers and tuned thresholds,\n",
    "- GNN checkpoints from Phase 03 (AF), by rebuilding the graph identically and loading best hyperparameters to match checkpoint shapes.\n",
    "\n",
    "### Outputs\n",
    "- `results/metrics/phase05_eval_report.json` — single JSON source of truth (metrics + paths).\n",
    "- `results/predictions/*.parquet` — per-model prediction exports (`txId, time_step, class, p_illicit, y_pred`).\n",
    "- `results/visualizations/phase05/phase05_final_f1_summary.png` — final illicit-F1 comparison bar plot.\n",
    "\n",
    "### Final result (this run)\n",
    "Best illicit-F1 on the held-out test window: **Random Forest (AF) with tuned threshold**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 05: extract a compact evaluation table (metrics + thresholds)\n",
    "phase05 = read_json(METRICS / \"phase05_eval_report.json\")\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "def _get(d, *keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "if phase05:\n",
    "    # Tabular tuned models (Phase 04 artifacts evaluated in Phase 05)\n",
    "    te = phase05.get(\"phase04_tabular_eval\", {})\n",
    "    for mode in (\"AF\",\"LF\"):\n",
    "        if mode in te:\n",
    "            for model_key, model_label in ((\"lr\",\"LR\"), (\"rf\",\"RF\")):\n",
    "                md = te[mode].get(model_key)\n",
    "                if not md:\n",
    "                    continue\n",
    "                metrics = md.get(\"metrics\", {})\n",
    "                thr = md.get(\"threshold\", md.get(\"best_threshold\", None))\n",
    "                rows.append({\n",
    "                    \"model\": f\"{model_label}_{mode}\",\n",
    "                    \"threshold\": thr,\n",
    "                    \"precision_illicit\": metrics.get(\"precision_illicit\"),\n",
    "                    \"recall_illicit\": metrics.get(\"recall_illicit\"),\n",
    "                    \"f1_illicit\": metrics.get(\"f1_illicit\"),\n",
    "                    \"f1_micro\": metrics.get(\"f1_micro\"),\n",
    "                })\n",
    "    # GNN checkpoints\n",
    "    for key, label in ((\"gcn_AF\",\"GCN_AF\"), (\"skip_gcn_AF\",\"SkipGCN_AF\")):\n",
    "        md = phase05.get(key)\n",
    "        if md and \"metrics\" in md:\n",
    "            m = md[\"metrics\"]\n",
    "            rows.append({\n",
    "                \"model\": label,\n",
    "                \"threshold\": None,\n",
    "                \"precision_illicit\": m.get(\"precision_illicit\"),\n",
    "                \"recall_illicit\": m.get(\"recall_illicit\"),\n",
    "                \"f1_illicit\": m.get(\"f1_illicit\"),\n",
    "                \"f1_micro\": m.get(\"f1_micro\"),\n",
    "            })\n",
    "\n",
    "\n",
    "df_phase05 = pd.DataFrame(rows).sort_values(\"f1_illicit\", ascending=False)\n",
    "df_phase05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phase05_dir = VIZ / \"phase05\"\n",
    "list_dir(phase05_dir, \"*.png\")\n",
    "show_image(phase05_dir / \"phase05_final_f1_summary.png\", title=\"phase05_final_f1_summary.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a RANDOM sample of prediction rows (with optional balancing so it's not all TN)\n",
    "# - If y_true / y_pred columns exist, it will try to show a mix of TP/FP/FN/TN.\n",
    "# - Falls back to uniform random sampling if columns are not found.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pred_files = list_dir(PRED, \"*.parquet\")\n",
    "\n",
    "preferred_order = [\n",
    "    \"phase05_tabular_rf_AF\",\n",
    "    \"phase05_tabular_rf_LF\",\n",
    "    \"phase05_tabular_lr_AF\",\n",
    "    \"phase05_tabular_lr_LF\",\n",
    "    \"phase05_gnn_skip_gcn_AF\",\n",
    "    \"phase05_gnn_gcn_AF\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "for pref in preferred_order:\n",
    "    for p in pred_files:\n",
    "        if pref in p.name:\n",
    "            chosen = p\n",
    "            break\n",
    "    if chosen:\n",
    "        break\n",
    "if chosen is None and pred_files:\n",
    "    chosen = pred_files[0]\n",
    "\n",
    "# ---- sampling helpers ----\n",
    "def _first_existing(cols, candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def sample_predictions(df: pd.DataFrame, n: int = 12, seed=None, balance_cm: bool = True) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    y_true_col = _first_existing(cols, [\"y_true\", \"label\", \"target\", \"true\", \"gt\", \"y\"])\n",
    "    y_pred_col = _first_existing(cols, [\"y_pred\", \"pred\", \"pred_label\", \"prediction\", \"yhat\"])\n",
    "    score_col  = _first_existing(cols, [\"y_score\", \"proba\", \"p_illicit\", \"prob\", \"score\", \"logit\"])\n",
    "\n",
    "    # If we can, create confusion-type strata and sample across them\n",
    "    if balance_cm and y_true_col and y_pred_col:\n",
    "        y_true = df[y_true_col]\n",
    "        y_pred = df[y_pred_col]\n",
    "\n",
    "        # try to coerce to 0/1\n",
    "        def to01(s):\n",
    "            if pd.api.types.is_bool_dtype(s):\n",
    "                return s.astype(int)\n",
    "            if pd.api.types.is_numeric_dtype(s):\n",
    "                return (s.astype(float) > 0.5).astype(int)\n",
    "            # string labels: map common tokens\n",
    "            m = s.astype(str).str.lower().map({\"0\":0,\"1\":1,\"licit\":0,\"illicit\":1,\"false\":0,\"true\":1})\n",
    "            if m.notna().any():\n",
    "                return m.fillna(0).astype(int)\n",
    "            return pd.to_numeric(s, errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        yt = to01(y_true)\n",
    "        yp = to01(y_pred)\n",
    "\n",
    "        strata = {\n",
    "            \"TP\": df[(yt == 1) & (yp == 1)],\n",
    "            \"FP\": df[(yt == 0) & (yp == 1)],\n",
    "            \"FN\": df[(yt == 1) & (yp == 0)],\n",
    "            \"TN\": df[(yt == 0) & (yp == 0)],\n",
    "        }\n",
    "\n",
    "        # allocate roughly equally, but don't exceed what's available\n",
    "        base = n // 4\n",
    "        rem = n % 4\n",
    "        order = [\"TP\", \"FP\", \"FN\", \"TN\"]\n",
    "        alloc = {k: base for k in order}\n",
    "        for k in order[:rem]:\n",
    "            alloc[k] += 1\n",
    "\n",
    "        parts = []\n",
    "        for k in order:\n",
    "            sdf = strata[k]\n",
    "            take = min(alloc[k], len(sdf))\n",
    "            if take > 0:\n",
    "                idx = rng.choice(sdf.index.to_numpy(), size=take, replace=False)\n",
    "                parts.append(df.loc[idx].assign(_bucket=k))\n",
    "        if parts:\n",
    "            out = pd.concat(parts, axis=0)\n",
    "            out = out.sample(frac=1.0, random_state=int(rng.integers(0, 2**32 - 1)))  # shuffle\n",
    "            # useful ordering: show score if exists\n",
    "            if score_col and score_col in out.columns:\n",
    "                out = out.sort_values(by=score_col, ascending=False)\n",
    "            return out.head(n)\n",
    "\n",
    "    # fallback: uniform random sample\n",
    "    idx = rng.choice(df.index.to_numpy(), size=min(n, len(df)), replace=False)\n",
    "    out = df.loc[idx]\n",
    "    if score_col and score_col in out.columns:\n",
    "        out = out.sort_values(by=score_col, ascending=False)\n",
    "    return out\n",
    "\n",
    "if chosen is not None:\n",
    "    print(\"Showing:\", chosen.relative_to(ROOT))\n",
    "    df_pred = pd.read_parquet(chosen)\n",
    "\n",
    "    # Change these knobs if you want:\n",
    "    N_SHOW = 12\n",
    "    BALANCE_CM = True          # set False for pure random\n",
    "    SEED = None                # set e.g. 42 for reproducible sampling\n",
    "\n",
    "    sample = sample_predictions(df_pred, n=N_SHOW, seed=SEED, balance_cm=BALANCE_CM)\n",
    "    display(sample)\n",
    "    print(\"rows:\", len(df_pred), \"cols:\", df_pred.shape[1])\n",
    "else:\n",
    "    print(\"[missing] no prediction parquet files found under results/predictions/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92667ae0",
   "metadata": {},
   "source": [
    "## 8) the differance between the paper scores and our experiments scores\n",
    "\n",
    "| Model    | Paper F1 | Our F1 | Δ (ours − paper) |\n",
    "| -------- | -------: | -----: | ---------------: |\n",
    "| LR (AF)  |    0.481 |  0.436 |           −0.045 |\n",
    "| LR (LF)  |    0.457 |  0.508 |           +0.051 |\n",
    "| RF (AF)  |    0.788 |  0.761 |           −0.027 |\n",
    "| RF (LF)  |    0.694 |  0.742 |           +0.048 |\n",
    "| GCN      |    0.628 |  0.509 |           −0.119 |\n",
    "| Skip-GCN |    0.705 |  0.521 |           −0.184 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a88794",
   "metadata": {},
   "source": [
    "## 9) Conclusions\n",
    "\n",
    "- The project implements an **end-to-end**, reproducible AML pipeline on Elliptic: preprocessing → EDA → baselines → GNNs → tuning → final evaluation.\n",
    "- **Temporal evaluation** (train on earlier `time_step`, test on later) is enforced throughout to prevent leakage and mimic deployment.\n",
    "- The dataset is **highly imbalanced** (illicit is the minority), so reporting focuses on **illicit precision/recall/F1**, not only accuracy/micro-F1.\n",
    "- **Tabular models on engineered features dominate** in illicit-F1 under this protocol; **AF** consistently outperforms **LF**.\n",
    "- **GNN baselines** (GCN / SkipGCN) learn usable signal but generalize to the test window with **moderate illicit-F1**, below tuned RF.\n",
    "- **Threshold tuning** (Phase 04) is required for a meaningful operating point under imbalance; Phase 05 evaluates models using these saved thresholds.\n",
    "- Prediction exports (Phase 05) enable auditability: the grader can inspect per-sample probabilities, threshold decisions, and errors by time step.\n",
    "- Main limitations: (i) limited GNN tuning budget/architecture depth, (ii) temporal distribution shift, (iii) unlabeled-node supervision sparsity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ca745",
   "metadata": {},
   "source": [
    "## Appendix A — How to reproduce end-to-end (commands)\n",
    "\n",
    "From repository root.\n",
    "\n",
    "### Option 1: run phase-by-phase (recommended for reproducibility)\n",
    "```bash\n",
    "# Phase 01\n",
    "python -m src.main --phase 1\n",
    "\n",
    "# Phase 02 (EDA)\n",
    "python -m src.main --phase 2\n",
    "\n",
    "# Phase 02b (normalization viz, optional)\n",
    "python -m src.main --phase 2b\n",
    "\n",
    "# Phase 03 (model selection)\n",
    "python -m src.main --phase 3 --device cuda --tabular-backend cuml\n",
    "\n",
    "# Phase 04 (tabular tuning + thresholding)\n",
    "python -m src.main --phase 4 --device cuda --tabular-backend cuml\n",
    "\n",
    "# Phase 05 (final eval + prediction export)\n",
    "python -m src.main --phase 5 --device cuda\n",
    "```\n",
    "\n",
    "### Key outputs to check\n",
    "- `results/metrics/phase01_data_summary.json`\n",
    "- `results/metrics/phase02_eda_summary.json`\n",
    "- `results/metrics/baselines_*.json`, `results/metrics/gnn_*.json`\n",
    "- `results/metrics/phase04_tuning_index_*.json`\n",
    "- `results/metrics/phase05_eval_report.json`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-elliptic-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
